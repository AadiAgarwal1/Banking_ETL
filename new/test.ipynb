{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mysql.connector\n",
    "import psycopg2\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "## from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7eba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(dotenv_path=r\"C:\\Users\\aadia\\OneDrive\\Desktop\\projects\\practice\\new\\env.env\")\n",
    "\n",
    "# Get DB config\n",
    "db_type = os.getenv(\"DB_TYPE\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "database = os.getenv(\"DB_NAME\")\n",
    "username = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "# jar_path = os.getenv(\"JAR_PATH\")\n",
    "table_list = os.getenv(\"DB_TABLES\").split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_db (db_type, host, port, database, username, password):\n",
    "    # Start Spark session\n",
    "    # spark = SparkSession.builder \\\n",
    "    #     .appName(\"Multi-Table SQL Connector\") \\\n",
    "    #     .config(\"spark.jars\", jar_path) \\\n",
    "    #     .getOrCreate()\n",
    "\n",
    "    # Choose driver and JDBC URL\n",
    "    if db_type == \"mysql\":\n",
    "        conn = mysql.connector.connect(host = host,port = port,user = username,password = password,database = database)\n",
    "    elif db_type == \"postgresql\":\n",
    "       conn = psycopg2.connect(host = host,port = port,user = username,password = password,dbname = database)\n",
    "    elif db_type == \"sqlserver\":\n",
    "        conn = pyodbc.connect(f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"f\"SERVER={host},{port};\"f\"DATABASE={database};\"f\"UID={username};\"f\"PWD={password}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported db_type\")\n",
    "\n",
    "    # # JDBC connection properties\n",
    "    # connection_properties = {\n",
    "    #     \"user\": username,\n",
    "    #     \"password\": password,\n",
    "    #     \"driver\": driver\n",
    "    # }\n",
    "\n",
    "    return conn\n",
    "\n",
    "def print_tables(table_list,conn):\n",
    "    # Load tables into a dictionary\n",
    "    dataframes = {}\n",
    "    for table in table_list:\n",
    "        # table = table.strip()\n",
    "        print(f\"ðŸ“¥ Loading table: {table}\")\n",
    "        # df = spark.read.jdbc(url=url, table=table, properties=connection_properties)\n",
    "        # df.createOrReplaceTempView(table)\n",
    "        # df.show(5)  # Preview\n",
    "        # dataframes[table] = df\n",
    "        cursor=conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table}\")\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        df = pd.DataFrame((rows),columns = columns)\n",
    "        # for row in rows:\n",
    "        #     print(row)\n",
    "        dataframes [table] = df\n",
    "        display(df)\n",
    "        cursor.close()\n",
    "        print()    \n",
    "    print()\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8988a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tables(dfs,join):\n",
    "    if join == \"inner\":\n",
    "        merge_table = pd.merge(dfs[\"Customers\"], dfs[\"Accounts\"], on = \"customer_id\", how = \"inner\")\n",
    "    elif join == \"left\":\n",
    "        merge_table = pd.merge(dfs[\"Customers\"], dfs[\"Accounts\"], on = \"customer_id\", how = \"left\")\n",
    "    elif join == \"right\":\n",
    "        merge_table = pd.merge(dfs[\"Customers\"], dfs[\"Accounts\"], on = \"customer_id\", how = \"right\")\n",
    "    elif join == \"outer\":\n",
    "        merge_table = pd.merge(dfs[\"Customers\"], dfs[\"Accounts\"], on = \"customer_id\", how = \"outer\")\n",
    "    else :\n",
    "        raise ValueError(\"Invalid join\")\n",
    "    return merge_table     \n",
    "\n",
    "\n",
    "def SQL_join(conn,join):\n",
    "    cursor = conn.cursor()\n",
    "    if join == \"inner\":\n",
    "        query = (\"SELECT Customers.*,Accounts.* FROM Customers INNER JOIN Accounts ON Customers.customer_id = Accounts.customer_id\")\n",
    "    elif join == \"left\":\n",
    "        query = (\"SELECT Customers.*,Accounts.* FROM Customers LEFT JOIN Accounts ON Customers.customer_id = Accounts.customer_id\")\n",
    "    elif join == \"right\":\n",
    "        query = (\"SELECT Customers.*,Accounts.* FROM Customers RIGHT JOIN Accounts ON Customers.customer_id = Accounts.customer_id\")\n",
    "    elif join == \"outer\":\n",
    "        query = (\"SELECT Customers.*,Accounts.* FROM Customers LEFT JOIN Accounts ON Customers.customer_id = Accounts.customer_id UNION SELECT Customers.*,Accounts.* FROM Customers RIGHT JOIN Accounts ON Customers.customer_id = Accounts.customer_id\")\n",
    "    else :\n",
    "        print(\"Invalid join\")\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    df = pd.DataFrame((rows),columns = columns)\n",
    "    cursor.close()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_aggregate_summary(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    sample = cursor.fetchall()\n",
    "    col_names = [desc[0] for desc in cursor.description]\n",
    "    df_sample = pd.DataFrame(sample, columns=col_names)\n",
    "\n",
    "    numeric_cols = df_sample.select_dtypes(include=['number']).columns.tolist()\n",
    "    string_cols = df_sample.select_dtypes(include=['object']).columns.tolist()\n",
    "    datetime_cols = df_sample.select_dtypes(include=['datetime64', 'datetime']).columns.tolist()\n",
    "\n",
    "    agg_expressions = []\n",
    "    col_metric_pairs = []\n",
    "\n",
    "    for col in col_names:\n",
    "        if col in numeric_cols:\n",
    "            metrics = ['COUNT', 'SUM', 'AVG', 'MIN', 'MAX']\n",
    "            funcs = [\n",
    "                f\"COUNT({col})\",\n",
    "                f\"SUM({col})\",\n",
    "                f\"AVG({col})\",\n",
    "                f\"MIN({col})\",\n",
    "                f\"MAX({col})\"\n",
    "            ]\n",
    "        elif col in string_cols:\n",
    "            metrics = ['COUNT', 'DISTINCT_COUNT', 'GROUP_CONCAT']\n",
    "            funcs = [\n",
    "                f\"COUNT({col})\",\n",
    "                f\"COUNT(DISTINCT {col})\",\n",
    "                f\"GROUP_CONCAT({col})\"\n",
    "            ]\n",
    "        elif col in datetime_cols:\n",
    "            metrics = ['COUNT', 'MIN', 'MAX']\n",
    "            funcs = [\n",
    "                f\"COUNT({col})\",\n",
    "                f\"MIN({col})\",\n",
    "                f\"MAX({col})\"\n",
    "            ]\n",
    "        else:\n",
    "            metrics = ['COUNT']\n",
    "            funcs = [f\"COUNT({col})\"]\n",
    "\n",
    "        agg_expressions.extend(funcs)\n",
    "        col_metric_pairs.extend([(col, m) for m in metrics])\n",
    "\n",
    "    # Final SQL\n",
    "    final_sql = f\"SELECT {', '.join(agg_expressions)} FROM {table_name}\"\n",
    "    cursor.execute(final_sql)\n",
    "    data = cursor.fetchone()\n",
    "    cursor.close()\n",
    "\n",
    "    summary_data = []\n",
    "    for (col, metric), value in zip(col_metric_pairs, data):\n",
    "        summary_data.append({\n",
    "            'column_name': col,\n",
    "            'metric_type': metric.lower(),\n",
    "            'value': value\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables into dictionary of DataFrames\n",
    "\n",
    "# spark , url,connection_properties = connect_and_load_tables(db_type, host, port, database, username, password, jar_path)\n",
    "# print('Reached')\n",
    "# dfs = create_df_from_tables(table_list,spark , url,connection_properties)\n",
    "conn = connect_db(db_type,host,port,database,username,password)\n",
    "print('Connected')\n",
    "dfs = {}\n",
    "dfs = print_tables(table_list,conn)\n",
    "# print (dfs)\n",
    "m_t = merge_tables(dfs,\"outer\")\n",
    "print(m_t)\n",
    "s_t =  SQL_join(conn,\"inner\")\n",
    "print(s_t)\n",
    "excel_writer = pd.ExcelWriter(\"aggregated_summary.xlsx\", engine='openpyxl')\n",
    "for i in table_list:\n",
    "    dp = sql_aggregate_summary(conn,i)\n",
    "    print(f\"\\nAggregated summary for table: {i}\")\n",
    "    print(dp)\n",
    "    \n",
    "    dp.to_csv(f\"{i}.csv\",index =False)\n",
    "    dp.to_excel(excel_writer, sheet_name=i, index=False)\n",
    "    dp[\"value\"] = dp[\"value\"].astype(str)\n",
    "    dp.to_parquet(f\"{i}.parquet\", index=False)\n",
    "\n",
    "excel_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) MERGE\n",
    "# 2) AGGREGATE FUNCTION (SUM,COUNT)\n",
    "# 3) MATPLOTLIB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
